# Подходы к задаче

## Файнтюн на корпусе

Пробовали, варьировать можно только модели и гиперпараметры обучения, получается неоч по лидерборду

## Fill in the middle

Был бейзлайном какого-то года. Сейчас на такое в основном предобучают кодеры. Вот такое сравнение кодовых моделей мне выдал дипсик: 

| Model              | Parameters | License          | Key Strength           | FIM-Specific? |
| ------------------ | ---------- | ---------------- | ---------------------- | ------------- |
| **StarCoder**      | 1B-15B     | BigCode OpenRAIL | Multi-language support | ✅ Yes         |
| **Code Llama**     | 7B-34B     | Community        | Long-context handling  | ✅ Yes         |
| **DeepSeek-Coder** | 1B-33B     | MIT              | Reasoning-heavy tasks  | ✅ Yes         |
| **Mistral-Code**   | 7B         | Apache 2.0       | Fast inference         | ✅ Yes         |

## Генерация синты / поиск новых датасетов

1. Детоксификация или токсиция комментариев с помощьею больших моделей

2. Генерация постов и комментариев на них 

3. Поиск похожих датасетов к нашей задаче 

## RL

Можно прикрупить какую нибудь Preference optimization чтобы генерить более хорошие по скору ответы, однако это как мне кажется сложно сделать в контексте этой задачи, т.к. хорошая детоксификация по скору может быть менее логичной для человека и LLM as a judge будет выбирать стремноватые примеры, тем более RL не завелся в прошлом году и дал какой-то копеечный прирост, но если есть свободные руки можно сделать. 

## Одна модель один язык

Тюним по машинке на язык, может получиться долго и дорого, но как вариант 

Плюсы подхода: 

1. Можно брать модели уже предобученные или натюненные под язык, учить на данных только этого языка, по идее качество должно улучшиться

2. Можно наращивать конкретные столбики метрик и не бояться потерять в остальных

3. Хорошо сочитается со всей предыдущей писаниной

Минусы подхода: 

    1. Минимум 15 обучений, хотя должны проходить быстро (кроме тех, где есть большие корпуса)

    2. На каждый язык свой датасет, хоть и небольшой, но нужно искать, либо генерить, либо адаптировать

## SAE

Есть серия таких [google/gemma-scope-2b-pt-res · Hugging Face](https://huggingface.co/google/gemma-scope-2b-pt-res) машинок. 

С помощью них можно: 

1. Определять что именно мы хотим закрашивать для FIM 

2. Менять токсичный токен на следующий самый вероятный логит при генерации 

3. Обнулять токсичные компоненты латентоного вектора, генерировать доброе продолжение, дальше можно оставлять концовку