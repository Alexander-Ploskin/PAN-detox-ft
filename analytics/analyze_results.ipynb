{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27fc3ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /home/alexander/dev/PAN-detox-ft/venv/lib/python3.12/site-packages (3.10.3)\n",
      "Requirement already satisfied: pandas in /home/alexander/dev/PAN-detox-ft/venv/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: seaborn in /home/alexander/dev/PAN-detox-ft/venv/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/alexander/dev/PAN-detox-ft/venv/lib/python3.12/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/alexander/dev/PAN-detox-ft/venv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/alexander/dev/PAN-detox-ft/venv/lib/python3.12/site-packages (from matplotlib) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/alexander/dev/PAN-detox-ft/venv/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/alexander/dev/PAN-detox-ft/venv/lib/python3.12/site-packages (from matplotlib) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/alexander/dev/PAN-detox-ft/venv/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/alexander/dev/PAN-detox-ft/venv/lib/python3.12/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/alexander/dev/PAN-detox-ft/venv/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/alexander/dev/PAN-detox-ft/venv/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/alexander/dev/PAN-detox-ft/venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/alexander/dev/PAN-detox-ft/venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/alexander/dev/PAN-detox-ft/venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib pandas seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d648cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c119214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_log_file(file_path):\n",
    "    \"\"\"Parse a single log file and extract metrics for each language.\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Extract model name from filename\n",
    "    model_name = os.path.basename(file_path).split('.')[0]\n",
    "    \n",
    "    # Extract metrics for each language\n",
    "    pattern = r\"Metrics for the language (\\w+): STA ([\\d.]+) SIM ([\\d.]+) CHRF ([\\d.]+) J ([\\d.]+) XCOMET ([\\d.]+)\"\n",
    "    matches = re.findall(pattern, content)\n",
    "    \n",
    "    # Extract average metrics\n",
    "    avg_j_pattern = r\"Average all J: ([\\d.]+)\"\n",
    "    avg_j = float(re.search(avg_j_pattern, content).group(1)) if re.search(avg_j_pattern, content) else None\n",
    "\n",
    "    avg_pj_pattern = r\"Average_p J: ([\\d.]+)\"\n",
    "    avg_pj = float(re.search(avg_pj_pattern, content).group(1)) if re.search(avg_pj_pattern, content) else None    \n",
    "\n",
    "    avg_npj_pattern = r\"Average_np J: ([\\d.]+)\"\n",
    "    avg_npj = float(re.search(avg_npj_pattern, content).group(1)) if re.search(avg_npj_pattern, content) else None    \n",
    "    \n",
    "    avg_xcomet_pattern = r\"Average XCOMET: ([\\d.]+)\"\n",
    "    avg_xcomet = float(re.search(avg_xcomet_pattern, content).group(1)) if re.search(avg_xcomet_pattern, content) else None\n",
    "    \n",
    "    results = []\n",
    "    for match in matches:\n",
    "        lang, sta, sim, chrf, j, xcomet = match\n",
    "        results.append({\n",
    "            'model': model_name,\n",
    "            'language': lang,\n",
    "            'STA': float(sta),\n",
    "            'SIM': float(sim),\n",
    "            'CHRF': float(chrf),\n",
    "            'J': float(j),\n",
    "            'XCOMET': float(xcomet)\n",
    "        })\n",
    "    \n",
    "    # Add average metrics as a separate entry\n",
    "    if avg_j is not None and avg_xcomet is not None:\n",
    "        results.append({\n",
    "            'model': model_name,\n",
    "            'language': 'average',\n",
    "            'STA': None,\n",
    "            'SIM': None,\n",
    "            'CHRF': None,\n",
    "            'J': avg_j,\n",
    "            'Average_p J': avg_pj,\n",
    "            'Average_np J': avg_npj,\n",
    "            'XCOMET': avg_xcomet\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfaea0d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model': 'en_comms_2000',\n",
       "  'language': 'am',\n",
       "  'STA': 0.802,\n",
       "  'SIM': 0.366,\n",
       "  'CHRF': 0.17,\n",
       "  'J': 0.134,\n",
       "  'XCOMET': 0.483},\n",
       " {'model': 'en_comms_2000',\n",
       "  'language': 'ar',\n",
       "  'STA': 0.893,\n",
       "  'SIM': 0.461,\n",
       "  'CHRF': 0.107,\n",
       "  'J': 0.236,\n",
       "  'XCOMET': 0.556},\n",
       " {'model': 'en_comms_2000',\n",
       "  'language': 'de',\n",
       "  'STA': 0.805,\n",
       "  'SIM': 0.662,\n",
       "  'CHRF': 0.329,\n",
       "  'J': 0.352,\n",
       "  'XCOMET': 0.679},\n",
       " {'model': 'en_comms_2000',\n",
       "  'language': 'en',\n",
       "  'STA': 0.921,\n",
       "  'SIM': 0.819,\n",
       "  'CHRF': 0.689,\n",
       "  'J': 0.654,\n",
       "  'XCOMET': 0.847},\n",
       " {'model': 'en_comms_2000',\n",
       "  'language': 'es',\n",
       "  'STA': 0.82,\n",
       "  'SIM': 0.656,\n",
       "  'CHRF': 0.299,\n",
       "  'J': 0.376,\n",
       "  'XCOMET': 0.689},\n",
       " {'model': 'en_comms_2000',\n",
       "  'language': 'fr',\n",
       "  'STA': 0.822,\n",
       "  'SIM': 0.688,\n",
       "  'CHRF': 0.341,\n",
       "  'J': 0.391,\n",
       "  'XCOMET': 0.693},\n",
       " {'model': 'en_comms_2000',\n",
       "  'language': 'he',\n",
       "  'STA': 0.821,\n",
       "  'SIM': 0.45,\n",
       "  'CHRF': 0.099,\n",
       "  'J': 0.192,\n",
       "  'XCOMET': 0.522},\n",
       " {'model': 'en_comms_2000',\n",
       "  'language': 'hi',\n",
       "  'STA': 0.771,\n",
       "  'SIM': 0.599,\n",
       "  'CHRF': 0.28,\n",
       "  'J': 0.261,\n",
       "  'XCOMET': 0.609},\n",
       " {'model': 'en_comms_2000',\n",
       "  'language': 'hin',\n",
       "  'STA': 0.665,\n",
       "  'SIM': 0.641,\n",
       "  'CHRF': 0.498,\n",
       "  'J': 0.26,\n",
       "  'XCOMET': 0.635},\n",
       " {'model': 'en_comms_2000',\n",
       "  'language': 'it',\n",
       "  'STA': 0.748,\n",
       "  'SIM': 0.698,\n",
       "  'CHRF': 0.441,\n",
       "  'J': 0.353,\n",
       "  'XCOMET': 0.689},\n",
       " {'model': 'en_comms_2000',\n",
       "  'language': 'ja',\n",
       "  'STA': 0.85,\n",
       "  'SIM': 0.508,\n",
       "  'CHRF': 0.197,\n",
       "  'J': 0.225,\n",
       "  'XCOMET': 0.548},\n",
       " {'model': 'en_comms_2000',\n",
       "  'language': 'ru',\n",
       "  'STA': 0.896,\n",
       "  'SIM': 0.509,\n",
       "  'CHRF': 0.074,\n",
       "  'J': 0.259,\n",
       "  'XCOMET': 0.56},\n",
       " {'model': 'en_comms_2000',\n",
       "  'language': 'tt',\n",
       "  'STA': 0.784,\n",
       "  'SIM': 0.466,\n",
       "  'CHRF': 0.305,\n",
       "  'J': 0.202,\n",
       "  'XCOMET': 0.577},\n",
       " {'model': 'en_comms_2000',\n",
       "  'language': 'uk',\n",
       "  'STA': 0.899,\n",
       "  'SIM': 0.507,\n",
       "  'CHRF': 0.12,\n",
       "  'J': 0.251,\n",
       "  'XCOMET': 0.557},\n",
       " {'model': 'en_comms_2000',\n",
       "  'language': 'zh',\n",
       "  'STA': 0.87,\n",
       "  'SIM': 0.471,\n",
       "  'CHRF': 0.066,\n",
       "  'J': 0.262,\n",
       "  'XCOMET': 0.621},\n",
       " {'model': 'en_comms_2000',\n",
       "  'language': 'average',\n",
       "  'STA': None,\n",
       "  'SIM': None,\n",
       "  'CHRF': None,\n",
       "  'J': 0.294,\n",
       "  'Average_p J': 0.309,\n",
       "  'Average_np J': 0.27,\n",
       "  'XCOMET': 0.618}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"/home/alexander/Desktop/submit_results/en_comms_2000.txt\"\n",
    "results = parse_log_file(file_path)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a5caeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_experiment_results(log_dir):\n",
    "    \"\"\"Analyze all experiment log files in the given directory.\"\"\"\n",
    "    log_files = glob(os.path.join(log_dir, \"*.txt\"))\n",
    "    \n",
    "    if not log_files:\n",
    "        print(f\"No log files found in {log_dir}\")\n",
    "        return None\n",
    "    \n",
    "    all_results = []\n",
    "    for file_path in log_files:\n",
    "        all_results.extend(parse_log_file(file_path))\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(all_results)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b73d5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>language</th>\n",
       "      <th>STA</th>\n",
       "      <th>SIM</th>\n",
       "      <th>CHRF</th>\n",
       "      <th>J</th>\n",
       "      <th>XCOMET</th>\n",
       "      <th>Average_p J</th>\n",
       "      <th>Average_np J</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en_comms_1000</td>\n",
       "      <td>am</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.403</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en_comms_1000</td>\n",
       "      <td>ar</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.376</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.523</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en_comms_1000</td>\n",
       "      <td>de</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.597</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.634</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en_comms_1000</td>\n",
       "      <td>en</td>\n",
       "      <td>0.921</td>\n",
       "      <td>0.819</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.849</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en_comms_1000</td>\n",
       "      <td>es</td>\n",
       "      <td>0.865</td>\n",
       "      <td>0.607</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.637</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>gemma_4b_2992_all_data_cleaned_final_lora</td>\n",
       "      <td>ru</td>\n",
       "      <td>0.917</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.673</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.888</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>gemma_4b_2992_all_data_cleaned_final_lora</td>\n",
       "      <td>tt</td>\n",
       "      <td>0.642</td>\n",
       "      <td>0.841</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.828</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>gemma_4b_2992_all_data_cleaned_final_lora</td>\n",
       "      <td>uk</td>\n",
       "      <td>0.915</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.883</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>gemma_4b_2992_all_data_cleaned_final_lora</td>\n",
       "      <td>zh</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.798</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.824</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>gemma_4b_2992_all_data_cleaned_final_lora</td>\n",
       "      <td>average</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>496 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         model language    STA    SIM   CHRF  \\\n",
       "0                                en_comms_1000       am  0.913  0.216  0.042   \n",
       "1                                en_comms_1000       ar  0.944  0.376  0.024   \n",
       "2                                en_comms_1000       de  0.888  0.597  0.230   \n",
       "3                                en_comms_1000       en  0.921  0.819  0.679   \n",
       "4                                en_comms_1000       es  0.865  0.607  0.221   \n",
       "..                                         ...      ...    ...    ...    ...   \n",
       "491  gemma_4b_2992_all_data_cleaned_final_lora       ru  0.917  0.837  0.673   \n",
       "492  gemma_4b_2992_all_data_cleaned_final_lora       tt  0.642  0.841  0.724   \n",
       "493  gemma_4b_2992_all_data_cleaned_final_lora       uk  0.915  0.857  0.700   \n",
       "494  gemma_4b_2992_all_data_cleaned_final_lora       zh  0.741  0.798  0.455   \n",
       "495  gemma_4b_2992_all_data_cleaned_final_lora  average    NaN    NaN    NaN   \n",
       "\n",
       "         J  XCOMET  Average_p J  Average_np J  \n",
       "0    0.079   0.403          NaN           NaN  \n",
       "1    0.200   0.523          NaN           NaN  \n",
       "2    0.345   0.634          NaN           NaN  \n",
       "3    0.655   0.849          NaN           NaN  \n",
       "4    0.348   0.637          NaN           NaN  \n",
       "..     ...     ...          ...           ...  \n",
       "491  0.692   0.888          NaN           NaN  \n",
       "492  0.456   0.828          NaN           NaN  \n",
       "493  0.700   0.883          NaN           NaN  \n",
       "494  0.489   0.824          NaN           NaN  \n",
       "495  0.581   0.848        0.602         0.548  \n",
       "\n",
       "[496 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_folder = \"/home/alexander/Desktop/submit_results\"\n",
    "all_results = analyze_experiment_results(results_folder)\n",
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd09fb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_models(df, metrics=None):\n",
    "    \"\"\"Find top models for each language and metric.\"\"\"\n",
    "    if metrics is None:\n",
    "        metrics = ['STA', 'SIM', 'CHRF', 'J', 'XCOMET']\n",
    "    \n",
    "    top_models = {}\n",
    "    \n",
    "    # For each language and metric, find the top model\n",
    "    for language in df['language'].unique():\n",
    "        top_models[language] = {}\n",
    "        lang_df = df[df['language'] == language]\n",
    "        \n",
    "        for metric in metrics:\n",
    "            if metric in lang_df.columns and not lang_df[metric].isna().all():\n",
    "                top_model = lang_df.loc[lang_df[metric].idxmax()]\n",
    "                top_models[language][metric] = {\n",
    "                    'model': top_model['model'],\n",
    "                    'value': top_model[metric]\n",
    "                }\n",
    "    \n",
    "    return top_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cae53d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'am': {'STA': {'model': 'en_comms_1000', 'value': np.float64(0.913)},\n",
       "  'SIM': {'model': 'gemma_4b_lora_paradetox', 'value': np.float64(0.81)},\n",
       "  'CHRF': {'model': 'baseline_delete_dev', 'value': np.float64(0.487)},\n",
       "  'J': {'model': 'gemma_4b_lora_paradetox', 'value': np.float64(0.461)},\n",
       "  'XCOMET': {'model': 'gemma_4b_lora_paradetox', 'value': np.float64(0.76)}},\n",
       " 'ar': {'STA': {'model': 'en_comms_1000', 'value': np.float64(0.944)},\n",
       "  'SIM': {'model': 'gemma_4b_lora_paradetox', 'value': np.float64(0.926)},\n",
       "  'CHRF': {'model': 'sorted_qwen2_tr_vanilla_paradetox_en_prompt_450_it__lr_2e-6',\n",
       "   'value': np.float64(0.786)},\n",
       "  'J': {'model': 'filter-88', 'value': np.float64(0.668)},\n",
       "  'XCOMET': {'model': 'gemma_4b_lora_paradetox', 'value': np.float64(0.89)}},\n",
       " 'de': {'STA': {'model': 'sorted_qwen2_tr_vanilla_paradetox_multi_prompt_338_it__lr_2e-6',\n",
       "   'value': np.float64(0.891)},\n",
       "  'SIM': {'model': 'gemma_4b_lora_paradetox', 'value': np.float64(0.946)},\n",
       "  'CHRF': {'model': 'sorted_qwen2_tr_vanilla_paradetox_multi_prompt_450_it__lr_2e-6',\n",
       "   'value': np.float64(0.819)},\n",
       "  'J': {'model': 'sorted_qwen2_tr_vanilla_paradetox_multi_prompt_338_it__lr_2e-6',\n",
       "   'value': np.float64(0.754)},\n",
       "  'XCOMET': {'model': 'gemma_4b_lora_paradetox', 'value': np.float64(0.959)}},\n",
       " 'en': {'STA': {'model': 'sorted_qwen2_tr_vanilla_paradetox_multi_prompt_338_it__lr_2e-6',\n",
       "   'value': np.float64(0.93)},\n",
       "  'SIM': {'model': 'gemma_4b_lora_paradetox', 'value': np.float64(0.892)},\n",
       "  'CHRF': {'model': 'sorted_qwen2_tr_vanilla_paradetox_multi_prompt_450_it__lr_2e-6',\n",
       "   'value': np.float64(0.73)},\n",
       "  'J': {'model': 'gemma_4b_2992_all_data_cleaned_final_lora',\n",
       "   'value': np.float64(0.704)},\n",
       "  'XCOMET': {'model': 'gemma_4b_2992_all_data_cleaned_final_lora',\n",
       "   'value': np.float64(0.89)}},\n",
       " 'es': {'STA': {'model': 'gemma_4b_1000_all_data_cleaned_final',\n",
       "   'value': np.float64(0.929)},\n",
       "  'SIM': {'model': 'gemma_4b_lora_paradetox', 'value': np.float64(0.887)},\n",
       "  'CHRF': {'model': 'baseline_delete_dev', 'value': np.float64(0.662)},\n",
       "  'J': {'model': 'gemma_4b_1000_all_data_cleaned_final',\n",
       "   'value': np.float64(0.698)},\n",
       "  'XCOMET': {'model': 'gemma_4b_lora_paradetox', 'value': np.float64(0.902)}},\n",
       " 'fr': {'STA': {'model': 'gemma_4b_1000_all_data_cleaned_final',\n",
       "   'value': np.float64(0.954)},\n",
       "  'SIM': {'model': 'gemma_4b_lora_paradetox', 'value': np.float64(0.942)},\n",
       "  'CHRF': {'model': 'sorted_qwen2_tr_vanilla_paradetox_multi_prompt_450_it__lr_2e-6',\n",
       "   'value': np.float64(0.859)},\n",
       "  'J': {'model': 'gemma_4b_1000_all_data_cleaned_final',\n",
       "   'value': np.float64(0.769)},\n",
       "  'XCOMET': {'model': 'gemma_4b_lora_paradetox', 'value': np.float64(0.92)}},\n",
       " 'he': {'STA': {'model': 'gemma_all_data', 'value': np.float64(0.915)},\n",
       "  'SIM': {'model': 'gemma_4b_lora_paradetox', 'value': np.float64(0.772)},\n",
       "  'CHRF': {'model': 'gemma_4b_1000_all_data_cleaned_final',\n",
       "   'value': np.float64(0.369)},\n",
       "  'J': {'model': 'gemma_4b_2992_all_data_cleaned_final_lora',\n",
       "   'value': np.float64(0.451)},\n",
       "  'XCOMET': {'model': 'gemma_4b_lora_paradetox', 'value': np.float64(0.814)}},\n",
       " 'hi': {'STA': {'model': 'en_comms_1000', 'value': np.float64(0.937)},\n",
       "  'SIM': {'model': 'gemma_4b_lora_paradetox', 'value': np.float64(0.889)},\n",
       "  'CHRF': {'model': 'baseline_delete_dev', 'value': np.float64(0.706)},\n",
       "  'J': {'model': 'gemma_4b_1000_all_data_cleaned_final',\n",
       "   'value': np.float64(0.577)},\n",
       "  'XCOMET': {'model': 'gemma_4b_lora_paradetox', 'value': np.float64(0.865)}},\n",
       " 'hin': {'STA': {'model': 'en_comms_1000', 'value': np.float64(0.724)},\n",
       "  'SIM': {'model': 'gemma_4b_lora_paradetox', 'value': np.float64(0.89)},\n",
       "  'CHRF': {'model': 'gemma_4b_lora_paradetox', 'value': np.float64(0.686)},\n",
       "  'J': {'model': 'sorted_qwen2_tr_vanilla_paradetox_multi_prompt_450_it__lr_2e-6',\n",
       "   'value': np.float64(0.455)},\n",
       "  'XCOMET': {'model': 'gemma_4b_lora_paradetox', 'value': np.float64(0.773)}},\n",
       " 'it': {'STA': {'model': 'gemma_4b_1000_all_data_cleaned_final',\n",
       "   'value': np.float64(0.948)},\n",
       "  'SIM': {'model': 'gemma_4b_lora_paradetox', 'value': np.float64(0.964)},\n",
       "  'CHRF': {'model': 'gemma_4b_lora_paradetox', 'value': np.float64(0.81)},\n",
       "  'J': {'model': 'gemma_4b_1000_all_data_cleaned_final',\n",
       "   'value': np.float64(0.755)},\n",
       "  'XCOMET': {'model': 'gemma_4b_lora_paradetox', 'value': np.float64(0.91)}},\n",
       " 'ja': {'STA': {'model': 'en_comms_1000', 'value': np.float64(0.958)},\n",
       "  'SIM': {'model': 'gemma_4b_lora_paradetox', 'value': np.float64(0.941)},\n",
       "  'CHRF': {'model': 'gemma_4b_lora_paradetox', 'value': np.float64(0.742)},\n",
       "  'J': {'model': 'gemma_4b_1000_all_data_cleaned_final',\n",
       "   'value': np.float64(0.589)},\n",
       "  'XCOMET': {'model': 'gemma_4b_lora_paradetox', 'value': np.float64(0.884)}},\n",
       " 'ru': {'STA': {'model': 'en_comms_1000', 'value': np.float64(0.948)},\n",
       "  'SIM': {'model': 'gemma_4b_lora_paradetox', 'value': np.float64(0.895)},\n",
       "  'CHRF': {'model': 'sorted_qwen2_tr_vanilla_paradetox_en_prompt_450_it__lr_2e-6',\n",
       "   'value': np.float64(0.74)},\n",
       "  'J': {'model': 'sorted_qwen2_tr_vanilla_paradetox_multi_prompt_338_it__lr_2e-6',\n",
       "   'value': np.float64(0.725)},\n",
       "  'XCOMET': {'model': 'gemma_4b_lora_paradetox', 'value': np.float64(0.899)}},\n",
       " 'tt': {'STA': {'model': 'en_comms_1000', 'value': np.float64(0.848)},\n",
       "  'SIM': {'model': 'gemma_4b_lora_paradetox', 'value': np.float64(0.904)},\n",
       "  'CHRF': {'model': 'baseline_delete_dev', 'value': np.float64(0.772)},\n",
       "  'J': {'model': 'baseline_delete_dev', 'value': np.float64(0.573)},\n",
       "  'XCOMET': {'model': 'gemma_4b_lora_paradetox', 'value': np.float64(0.842)}},\n",
       " 'uk': {'STA': {'model': 'sorted_qwen2_tr_vanilla_paradetox_multi_prompt_225_it__lr_1e-5',\n",
       "   'value': np.float64(0.948)},\n",
       "  'SIM': {'model': 'gemma_4b_lora_paradetox', 'value': np.float64(0.94)},\n",
       "  'CHRF': {'model': 'uk_comms_487', 'value': np.float64(0.813)},\n",
       "  'J': {'model': 'sorted_qwen2_tr_vanilla_paradetox_multi_prompt_338_it__lr_2e-6',\n",
       "   'value': np.float64(0.766)},\n",
       "  'XCOMET': {'model': 'gemma_4b_lora_paradetox', 'value': np.float64(0.91)}},\n",
       " 'zh': {'STA': {'model': 'en_comms_1000', 'value': np.float64(0.949)},\n",
       "  'SIM': {'model': 'gemma_4b_lora_paradetox', 'value': np.float64(0.874)},\n",
       "  'CHRF': {'model': 'sorted_qwen2_tr_vanilla_paradetox_multi_prompt_450_it__lr_2e-6',\n",
       "   'value': np.float64(0.65)},\n",
       "  'J': {'model': 'sorted_qwen2_tr_vanilla_paradetox_en_prompt_450_it__lr_2e-6',\n",
       "   'value': np.float64(0.531)},\n",
       "  'XCOMET': {'model': 'gemma_4b_lora_paradetox', 'value': np.float64(0.857)}},\n",
       " 'average': {'J': {'model': 'gemma_4b_1000_all_data_cleaned_final',\n",
       "   'value': np.float64(0.616)},\n",
       "  'XCOMET': {'model': 'gemma_4b_lora_paradetox', 'value': np.float64(0.862)}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_top_models(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3ec0e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_by_language(df, metric, output_dir=None):\n",
    "    \"\"\"Create a bar plot for a specific metric across languages and models.\"\"\"\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Filter out rows where the metric is NaN\n",
    "    filtered_df = df.dropna(subset=[metric])\n",
    "    \n",
    "    # Create the plot\n",
    "    ax = sns.barplot(x='language', y=metric, hue='model', data=filtered_df)\n",
    "    \n",
    "    plt.title(f'{metric} Scores by Language and Model')\n",
    "    plt.xlabel('Language')\n",
    "    plt.ylabel(f'{metric} Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        plt.savefig(os.path.join(output_dir, f'{metric}_by_language.png'))\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_average_metrics(df, output_dir=None):\n",
    "    \"\"\"Create a bar plot for average metrics across models.\"\"\"\n",
    "    avg_df = df[df['language'] == 'average']\n",
    "    \n",
    "    if avg_df.empty:\n",
    "        print(\"No average metrics found in the data\")\n",
    "        return\n",
    "    \n",
    "    # Melt the dataframe to get metrics in a single column\n",
    "    melted_df = pd.melt(\n",
    "        avg_df, \n",
    "        id_vars=['model'], \n",
    "        value_vars=['J', 'XCOMET'],\n",
    "        var_name='metric', \n",
    "        value_name='score'\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    ax = sns.barplot(x='model', y='score', hue='metric', data=melted_df)\n",
    "    \n",
    "    plt.title('Average Metrics by Model')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Metric')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        plt.savefig(os.path.join(output_dir, 'average_metrics.png'))\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_heatmap(df, metric, output_dir=None):\n",
    "    \"\"\"Create a heatmap of a specific metric for all models and languages.\"\"\"\n",
    "    # Pivot the dataframe to get models as rows and languages as columns\n",
    "    pivot_df = df.pivot_table(index='model', columns='language', values=metric)\n",
    "    \n",
    "    plt.figure(figsize=(16, 10))\n",
    "    ax = sns.heatmap(pivot_df, annot=True, fmt=\".3f\", cmap=\"YlGnBu\", linewidths=.5)\n",
    "    \n",
    "    plt.title(f'{metric} Scores Heatmap')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        plt.savefig(os.path.join(output_dir, f'{metric}_heatmap.png'))\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "575598b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Models by Language and Metric:\n",
      "\n",
      "Language: am\n",
      "  STA: en_comms_1000 (0.913)\n",
      "  SIM: gemma_4b_lora_paradetox (0.810)\n",
      "  CHRF: baseline_delete_dev (0.487)\n",
      "  J: gemma_4b_lora_paradetox (0.461)\n",
      "  XCOMET: gemma_4b_lora_paradetox (0.760)\n",
      "\n",
      "Language: ar\n",
      "  STA: en_comms_1000 (0.944)\n",
      "  SIM: gemma_4b_lora_paradetox (0.926)\n",
      "  CHRF: sorted_qwen2_tr_vanilla_paradetox_en_prompt_450_it__lr_2e-6 (0.786)\n",
      "  J: filter-88 (0.668)\n",
      "  XCOMET: gemma_4b_lora_paradetox (0.890)\n",
      "\n",
      "Language: de\n",
      "  STA: sorted_qwen2_tr_vanilla_paradetox_multi_prompt_338_it__lr_2e-6 (0.891)\n",
      "  SIM: gemma_4b_lora_paradetox (0.946)\n",
      "  CHRF: sorted_qwen2_tr_vanilla_paradetox_multi_prompt_450_it__lr_2e-6 (0.819)\n",
      "  J: sorted_qwen2_tr_vanilla_paradetox_multi_prompt_338_it__lr_2e-6 (0.754)\n",
      "  XCOMET: gemma_4b_lora_paradetox (0.959)\n",
      "\n",
      "Language: en\n",
      "  STA: sorted_qwen2_tr_vanilla_paradetox_multi_prompt_338_it__lr_2e-6 (0.930)\n",
      "  SIM: gemma_4b_lora_paradetox (0.892)\n",
      "  CHRF: sorted_qwen2_tr_vanilla_paradetox_multi_prompt_450_it__lr_2e-6 (0.730)\n",
      "  J: gemma_4b_2992_all_data_cleaned_final_lora (0.704)\n",
      "  XCOMET: gemma_4b_2992_all_data_cleaned_final_lora (0.890)\n",
      "\n",
      "Language: es\n",
      "  STA: gemma_4b_1000_all_data_cleaned_final (0.929)\n",
      "  SIM: gemma_4b_lora_paradetox (0.887)\n",
      "  CHRF: baseline_delete_dev (0.662)\n",
      "  J: gemma_4b_1000_all_data_cleaned_final (0.698)\n",
      "  XCOMET: gemma_4b_lora_paradetox (0.902)\n",
      "\n",
      "Language: fr\n",
      "  STA: gemma_4b_1000_all_data_cleaned_final (0.954)\n",
      "  SIM: gemma_4b_lora_paradetox (0.942)\n",
      "  CHRF: sorted_qwen2_tr_vanilla_paradetox_multi_prompt_450_it__lr_2e-6 (0.859)\n",
      "  J: gemma_4b_1000_all_data_cleaned_final (0.769)\n",
      "  XCOMET: gemma_4b_lora_paradetox (0.920)\n",
      "\n",
      "Language: he\n",
      "  STA: gemma_all_data (0.915)\n",
      "  SIM: gemma_4b_lora_paradetox (0.772)\n",
      "  CHRF: gemma_4b_1000_all_data_cleaned_final (0.369)\n",
      "  J: gemma_4b_2992_all_data_cleaned_final_lora (0.451)\n",
      "  XCOMET: gemma_4b_lora_paradetox (0.814)\n",
      "\n",
      "Language: hi\n",
      "  STA: en_comms_1000 (0.937)\n",
      "  SIM: gemma_4b_lora_paradetox (0.889)\n",
      "  CHRF: baseline_delete_dev (0.706)\n",
      "  J: gemma_4b_1000_all_data_cleaned_final (0.577)\n",
      "  XCOMET: gemma_4b_lora_paradetox (0.865)\n",
      "\n",
      "Language: hin\n",
      "  STA: en_comms_1000 (0.724)\n",
      "  SIM: gemma_4b_lora_paradetox (0.890)\n",
      "  CHRF: gemma_4b_lora_paradetox (0.686)\n",
      "  J: sorted_qwen2_tr_vanilla_paradetox_multi_prompt_450_it__lr_2e-6 (0.455)\n",
      "  XCOMET: gemma_4b_lora_paradetox (0.773)\n",
      "\n",
      "Language: it\n",
      "  STA: gemma_4b_1000_all_data_cleaned_final (0.948)\n",
      "  SIM: gemma_4b_lora_paradetox (0.964)\n",
      "  CHRF: gemma_4b_lora_paradetox (0.810)\n",
      "  J: gemma_4b_1000_all_data_cleaned_final (0.755)\n",
      "  XCOMET: gemma_4b_lora_paradetox (0.910)\n",
      "\n",
      "Language: ja\n",
      "  STA: en_comms_1000 (0.958)\n",
      "  SIM: gemma_4b_lora_paradetox (0.941)\n",
      "  CHRF: gemma_4b_lora_paradetox (0.742)\n",
      "  J: gemma_4b_1000_all_data_cleaned_final (0.589)\n",
      "  XCOMET: gemma_4b_lora_paradetox (0.884)\n",
      "\n",
      "Language: ru\n",
      "  STA: en_comms_1000 (0.948)\n",
      "  SIM: gemma_4b_lora_paradetox (0.895)\n",
      "  CHRF: sorted_qwen2_tr_vanilla_paradetox_en_prompt_450_it__lr_2e-6 (0.740)\n",
      "  J: sorted_qwen2_tr_vanilla_paradetox_multi_prompt_338_it__lr_2e-6 (0.725)\n",
      "  XCOMET: gemma_4b_lora_paradetox (0.899)\n",
      "\n",
      "Language: tt\n",
      "  STA: en_comms_1000 (0.848)\n",
      "  SIM: gemma_4b_lora_paradetox (0.904)\n",
      "  CHRF: baseline_delete_dev (0.772)\n",
      "  J: baseline_delete_dev (0.573)\n",
      "  XCOMET: gemma_4b_lora_paradetox (0.842)\n",
      "\n",
      "Language: uk\n",
      "  STA: sorted_qwen2_tr_vanilla_paradetox_multi_prompt_225_it__lr_1e-5 (0.948)\n",
      "  SIM: gemma_4b_lora_paradetox (0.940)\n",
      "  CHRF: uk_comms_487 (0.813)\n",
      "  J: sorted_qwen2_tr_vanilla_paradetox_multi_prompt_338_it__lr_2e-6 (0.766)\n",
      "  XCOMET: gemma_4b_lora_paradetox (0.910)\n",
      "\n",
      "Language: zh\n",
      "  STA: en_comms_1000 (0.949)\n",
      "  SIM: gemma_4b_lora_paradetox (0.874)\n",
      "  CHRF: sorted_qwen2_tr_vanilla_paradetox_multi_prompt_450_it__lr_2e-6 (0.650)\n",
      "  J: sorted_qwen2_tr_vanilla_paradetox_en_prompt_450_it__lr_2e-6 (0.531)\n",
      "  XCOMET: gemma_4b_lora_paradetox (0.857)\n",
      "\n",
      "Language: average\n",
      "  J: gemma_4b_1000_all_data_cleaned_final (0.616)\n",
      "  XCOMET: gemma_4b_lora_paradetox (0.862)\n"
     ]
    }
   ],
   "source": [
    "log_dir = \"/home/alexander/Desktop/submit_results\"\n",
    "output_dir = \".\"\n",
    "\n",
    "# Analyze results\n",
    "results_df = analyze_experiment_results(log_dir)\n",
    "\n",
    "# Find top models for each language and metric\n",
    "top_models = find_top_models(results_df)\n",
    "\n",
    "# Print top models for each language and metric\n",
    "print(\"Top Models by Language and Metric:\")\n",
    "for language, metrics in top_models.items():\n",
    "    print(f\"\\nLanguage: {language}\")\n",
    "    for metric, model_info in metrics.items():\n",
    "        print(f\"  {metric}: {model_info['model']} ({model_info['value']:.3f})\")\n",
    "\n",
    "# Plot metrics by language\n",
    "for metric in ['STA', 'SIM', 'CHRF', 'J', 'XCOMET']:\n",
    "    plot_metrics_by_language(results_df, metric, output_dir)\n",
    "\n",
    "# Plot average metrics\n",
    "plot_average_metrics(results_df, output_dir)\n",
    "\n",
    "# Create heatmaps for J and XCOMET metrics\n",
    "plot_heatmap(results_df, 'J', output_dir)\n",
    "plot_heatmap(results_df, 'XCOMET', output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b062ac65",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"/home/alexander/Desktop/submit_results\"\n",
    "tsv_dir = \"/home/alexander/Desktop/submit_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a77fece1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file 'best_possible_submit.tsv' created with 9000 rows.\n",
      "\n",
      "Language distribution in submission:\n",
      "uk: 600 rows\n",
      "hi: 600 rows\n",
      "zh: 600 rows\n",
      "ar: 600 rows\n",
      "de: 600 rows\n",
      "en: 600 rows\n",
      "ru: 600 rows\n",
      "am: 600 rows\n",
      "es: 600 rows\n",
      "it: 600 rows\n",
      "fr: 600 rows\n",
      "he: 600 rows\n",
      "hin: 600 rows\n",
      "tt: 600 rows\n",
      "ja: 600 rows\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Parse log files to get best model per language by J\n",
    "results = []\n",
    "log_files = glob(os.path.join(log_dir, \"*.txt\"))\n",
    "for log_path in log_files:\n",
    "    model_name = os.path.splitext(os.path.basename(log_path))[0]\n",
    "    with open(log_path, encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    for m in re.findall(r\"Metrics for the language (\\w+):.*?J ([\\d.]+)\", content):\n",
    "        lang, j = m\n",
    "        results.append({'model': model_name, 'language': lang, 'J': float(j)})\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "if df.empty:\n",
    "    raise Exception(\"No results found in log files.\")\n",
    "\n",
    "best_models = df.loc[df.groupby('language')['J'].idxmax()].reset_index(drop=True)\n",
    "\n",
    "# Step 2: Create a mapping of language to best model\n",
    "lang_to_best_model = dict(zip(best_models['language'], best_models['model']))\n",
    "\n",
    "# Step 3: Find a reference .tsv file to determine the correct order\n",
    "# We'll use the first .tsv file as a reference for order\n",
    "tsv_files = glob(os.path.join(tsv_dir, \"*.tsv\"))\n",
    "if not tsv_files:\n",
    "    raise Exception(\"No .tsv files found.\")\n",
    "\n",
    "# Load the reference file to get the order\n",
    "reference_file = tsv_files[0]\n",
    "reference_df = pd.read_csv(reference_file, sep='\\t')\n",
    "\n",
    "# Create an order mapping: language -> list of rows in order\n",
    "language_order = {}\n",
    "for i, row in reference_df.iterrows():\n",
    "    lang = row['lang']\n",
    "    if lang not in language_order:\n",
    "        language_order[lang] = []\n",
    "    language_order[lang].append(i)\n",
    "\n",
    "# Step 4: Load all .tsv files into a dictionary keyed by model name\n",
    "model_tsvs = {}\n",
    "for tsv_path in tsv_files:\n",
    "    model_name = os.path.splitext(os.path.basename(tsv_path))[0]\n",
    "    model_tsvs[model_name] = pd.read_csv(tsv_path, sep='\\t')\n",
    "\n",
    "# Step 5: Create the final submission by selecting rows from the best model for each language\n",
    "# while preserving the original order\n",
    "final_rows = []\n",
    "\n",
    "# Process each language in the order it appears in the reference file\n",
    "processed_langs = set()\n",
    "for i, row in reference_df.iterrows():\n",
    "    lang = row['lang']\n",
    "    \n",
    "    # Skip if we've already processed this language\n",
    "    if lang in processed_langs:\n",
    "        continue\n",
    "    \n",
    "    # Get the best model for this language\n",
    "    best_model = lang_to_best_model.get(lang)\n",
    "    if not best_model:\n",
    "        print(f\"Warning: No best model found for language {lang}\")\n",
    "        continue\n",
    "    \n",
    "    # Get the .tsv for the best model\n",
    "    if best_model not in model_tsvs:\n",
    "        print(f\"Warning: No .tsv file for model {best_model}\")\n",
    "        continue\n",
    "    \n",
    "    # Get all rows for this language from the best model\n",
    "    lang_df = model_tsvs[best_model][model_tsvs[best_model]['lang'] == lang]\n",
    "    if lang_df.empty:\n",
    "        print(f\"Warning: No rows for language {lang} in model {best_model}'s .tsv\")\n",
    "        continue\n",
    "    \n",
    "    # Add to final rows\n",
    "    final_rows.append(lang_df)\n",
    "    processed_langs.add(lang)\n",
    "\n",
    "# Step 6: Concatenate and save the submission\n",
    "if final_rows:\n",
    "    submission_df = pd.concat(final_rows, ignore_index=True)\n",
    "    submission_df.to_csv(\"best_possible_submit.tsv\", sep='\\t', index=False)\n",
    "    print(f\"Submission file 'best_possible_submit.tsv' created with {len(submission_df)} rows.\")\n",
    "    \n",
    "    # Print statistics about languages in the submission\n",
    "    lang_counts = submission_df['lang'].value_counts()\n",
    "    print(\"\\nLanguage distribution in submission:\")\n",
    "    for lang, count in lang_counts.items():\n",
    "        print(f\"{lang}: {count} rows\")\n",
    "else:\n",
    "    print(\"No data to submit.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123de260",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
