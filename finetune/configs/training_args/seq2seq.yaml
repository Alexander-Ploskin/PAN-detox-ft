# config/seq2seq_training_args.yaml
_target_: transformers.Seq2SeqTrainingArguments

# Required parameters
output_dir: "mt5"  # Directory where outputs will be saved

# Training parameters
per_device_train_batch_size: 8
per_device_eval_batch_size: 8
gradient_accumulation_steps: 1
learning_rate: 3e-4
weight_decay: 0.01
num_train_epochs: 1
max_steps: -1  # -1 means use num_train_epochs instead
#lr_scheduler_type: "linear"
#warmup_steps: 0
#max_length : 128
# Evaluation parameters
#eval_strategy: "steps"
#eval_steps: 10
logging_strategy: "steps"
logging_steps: 10

# Saving parameters
save_strategy: "steps"
save_steps: 500
save_total_limit: 2  # Maximum number of checkpoints to keep

# Mixed precision training
fp16: false
bf16: false
lr_scheduler_kwargs: null
# Other parameters
seed: 42
remove_unused_columns: true
load_best_model_at_end: false
predict_with_generate: false  # Important for seq2seq tasks
report_to: "wandb"  # or "wandb" if using Weights & Biases
run_name : 'mt5_baseline'
# Generation parameters (for predict_with_generate)
#generation_max_length: null
#generation_num_beams: null